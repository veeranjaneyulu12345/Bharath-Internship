{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction\n","\n","When it comes to property values, it is common to hear the phrase, \"location, location, location\", as the variable that dominates in influence on overall home value. But what other factors come into play in determining the price of a home? These factors are examined in this dataset. My prediction is that variables such as YearBuilt, OverallQual, OverallCond, Neighborhood, GrLivArea, KitchenQual will play the biggest roles in determining SalePrice."]},{"cell_type":"markdown","metadata":{},"source":["# Analysis Strategy\n","\n","The following is the analysis strategy to predict SalesPrice and determine which variables play the biggest roles in SalePrice. As the final step, various modeling techniques will be used on the entire dataset (without features that had too many missing values).\n","\n","1. Clean the data\n","    - Process and summarize data\n","    - Handle duplicate and missing data\n","    - Remove irrelevant data\n","\n","2. EDA Plan\n","    - Determine quantitative and qualitative features\n","    - Encode qualitative features\n","    - Determine most important features using heat maps\n","    - Analyze dependent variable, SalePrice\n","\n","3. Modeling\n","    - Linear Regression\n","    - Lasso Regression\n","    - Ridge Regression\n","    - Gradient Boosting and XGBoost \n","    \n","4. Conclusions\n","5. References"]},{"cell_type":"markdown","metadata":{},"source":["# Imports and Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.348368Z","iopub.status.busy":"2021-07-28T18:41:02.347873Z","iopub.status.idle":"2021-07-28T18:41:02.353598Z","shell.execute_reply":"2021-07-28T18:41:02.351618Z","shell.execute_reply.started":"2021-07-28T18:41:02.348334Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.355952Z","iopub.status.busy":"2021-07-28T18:41:02.355463Z","iopub.status.idle":"2021-07-28T18:41:02.413571Z","shell.execute_reply":"2021-07-28T18:41:02.412749Z","shell.execute_reply.started":"2021-07-28T18:41:02.355908Z"},"trusted":true},"outputs":[],"source":["test = pd.read_csv('../input/housepricesdata/test.csv')\n","train = pd.read_csv('../input/housepricesdata/train.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Data Processing and Summarizing Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.415715Z","iopub.status.busy":"2021-07-28T18:41:02.415131Z","iopub.status.idle":"2021-07-28T18:41:02.42352Z","shell.execute_reply":"2021-07-28T18:41:02.422386Z","shell.execute_reply.started":"2021-07-28T18:41:02.415672Z"},"trusted":true},"outputs":[],"source":["print ('Train dataframe: ', train.shape[0],'houses, and ', train.shape[1],'features')\n","print ('Test dataframe: ', test.shape[0],'houses, and ', test.shape[1],'features')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.425722Z","iopub.status.busy":"2021-07-28T18:41:02.425203Z","iopub.status.idle":"2021-07-28T18:41:02.453954Z","shell.execute_reply":"2021-07-28T18:41:02.452876Z","shell.execute_reply.started":"2021-07-28T18:41:02.425682Z"},"trusted":true},"outputs":[],"source":["# Determine quantitative and qualitative features\n","quantitative = [i for i in train.columns if train.dtypes[i] != 'object']\n","quantitative.remove('SalePrice')\n","quantitative.remove('Id')\n","\n","qualitative = [i for i in train.columns if train.dtypes[i] == 'object']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.455527Z","iopub.status.busy":"2021-07-28T18:41:02.455189Z","iopub.status.idle":"2021-07-28T18:41:02.460148Z","shell.execute_reply":"2021-07-28T18:41:02.45946Z","shell.execute_reply.started":"2021-07-28T18:41:02.455497Z"},"trusted":true},"outputs":[],"source":["print(list(quantitative))\n","print()\n","print('Train dataframe has: ', len(quantitative), 'quantitative features')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.461754Z","iopub.status.busy":"2021-07-28T18:41:02.461235Z","iopub.status.idle":"2021-07-28T18:41:02.473005Z","shell.execute_reply":"2021-07-28T18:41:02.472148Z","shell.execute_reply.started":"2021-07-28T18:41:02.461724Z"},"trusted":true},"outputs":[],"source":["print(list(qualitative))\n","print()\n","print('Train dataframe has: ', len(qualitative), 'qualitative features')"]},{"cell_type":"markdown","metadata":{},"source":["There are 1460 records of training data and 81 attributes/features. 36 of these features are quantitative and 43 are categorical. The next step is to take a deeper dive into all of these features to determine which of these may weaken our analysis for reasons such as large amounts of missing data. It is also necessary to develop a strategy for the categorical features to make them usable in our models."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.475259Z","iopub.status.busy":"2021-07-28T18:41:02.474485Z","iopub.status.idle":"2021-07-28T18:41:02.502751Z","shell.execute_reply":"2021-07-28T18:41:02.501734Z","shell.execute_reply.started":"2021-07-28T18:41:02.475221Z"},"trusted":true},"outputs":[],"source":["train.info()"]},{"cell_type":"markdown","metadata":{},"source":["At a glance, various features are missing large amounts of values. Thankfully, the columns of interest for our prediction are pretty complete. Missing values will be examined further below and further analysis will be performed to determine variables that are most highly correlated with SalePrice."]},{"cell_type":"markdown","metadata":{},"source":["# Handling Duplicate Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.504968Z","iopub.status.busy":"2021-07-28T18:41:02.504693Z","iopub.status.idle":"2021-07-28T18:41:02.516775Z","shell.execute_reply":"2021-07-28T18:41:02.515066Z","shell.execute_reply.started":"2021-07-28T18:41:02.50494Z"},"trusted":true},"outputs":[],"source":["# Check for duplicates\n","train['Id'].value_counts().sort_values(ascending = False)"]},{"cell_type":"markdown","metadata":{},"source":["There are no duplicate house entries in the dataset."]},{"cell_type":"markdown","metadata":{},"source":["# Handling Missing Data"]},{"cell_type":"markdown","metadata":{},"source":["It is important to understand how much missing data exists in the dataset, where they exists, and whether they are missing at random or systemically. Understanding this will affect the strategy used to handle missing data and can help reveal whether bias exists."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.519644Z","iopub.status.busy":"2021-07-28T18:41:02.519228Z","iopub.status.idle":"2021-07-28T18:41:02.784461Z","shell.execute_reply":"2021-07-28T18:41:02.783442Z","shell.execute_reply.started":"2021-07-28T18:41:02.519601Z"},"trusted":true},"outputs":[],"source":["# Visualize missing data\n","sns.set_style('ticks')\n","missing = train.isnull().sum()\n","missing = missing[missing > 0]\n","missing.sort_values(ascending=False, inplace=True)\n","missing.plot.bar()\n","\n","print(missing)\n","print('The number of features with missing data: %i' % missing.count())"]},{"cell_type":"markdown","metadata":{},"source":["The index range is 1460 with a total of 81 columns. Columns with more than 200 missing data can be removed as that is approximately 15% of the data missing. Attempting to fill these data will result in making these features weak predictors. It is better to leave them out entirely. \n","\n","The various Garage- features can also be deleted. They don't seem to be features that would contribute heavily to the overall house price. The feature, 'GarageCars', is likely more correlated with the house price given that the number of cars that are able to be parked in the garage is the most important aspect of the garage size. \n","\n","The Bsmnt- features could also be deleted given that the most important information is contained in the 'TotalBsmtSF' feature. \n","\n","The MasVnr- features are not important as they don't show strong correlations with any of the other features in the heat map. \n","\n","For 'Electrical', since there is only one record that is missing, the record can be filled with the most common value for this feature."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.78762Z","iopub.status.busy":"2021-07-28T18:41:02.787293Z","iopub.status.idle":"2021-07-28T18:41:02.793606Z","shell.execute_reply":"2021-07-28T18:41:02.79297Z","shell.execute_reply.started":"2021-07-28T18:41:02.78759Z"},"trusted":true},"outputs":[],"source":["# Inspect the values for Electrical\n","print(train['Electrical'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.794892Z","iopub.status.busy":"2021-07-28T18:41:02.794567Z","iopub.status.idle":"2021-07-28T18:41:02.828876Z","shell.execute_reply":"2021-07-28T18:41:02.82781Z","shell.execute_reply.started":"2021-07-28T18:41:02.794863Z"},"trusted":true},"outputs":[],"source":["# Drop missing data\n","total = train.isnull().sum().sort_values(ascending=False)\n","percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n","missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n","\n","train = train.drop((missing_data[missing_data['Total'] > 1]).index, 1)\n","\n","# Fill missing Electrical data\n","train['Electrical'] = train['Electrical'].fillna(train['Electrical'].value_counts().index[0])\n","\n","# Display sum of features with missing data\n","train.isnull().sum().max()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.830188Z","iopub.status.busy":"2021-07-28T18:41:02.829941Z","iopub.status.idle":"2021-07-28T18:41:02.835615Z","shell.execute_reply":"2021-07-28T18:41:02.834951Z","shell.execute_reply.started":"2021-07-28T18:41:02.830163Z"},"trusted":true},"outputs":[],"source":["# Inspect the values for Electrical after handling missing value\n","print(train['Electrical'].value_counts())"]},{"cell_type":"markdown","metadata":{},"source":["# Determining Most Important Variables "]},{"cell_type":"markdown","metadata":{},"source":["Since there are both categorical and quantitative variables in the dataset, it's necessary to come up with a strategy to quantify the categorical features so that they can be included in the analysis and later in the models. Categorical features will be encoded using a dictionary to map all of the unique values to numbers that can be used in the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.836955Z","iopub.status.busy":"2021-07-28T18:41:02.836611Z","iopub.status.idle":"2021-07-28T18:41:02.862504Z","shell.execute_reply":"2021-07-28T18:41:02.861676Z","shell.execute_reply.started":"2021-07-28T18:41:02.83693Z"},"trusted":true},"outputs":[],"source":["quantitative = [i for i in train.columns if train.dtypes[i] != 'object']\n","quantitative.remove('SalePrice')\n","quantitative.remove('Id')\n","\n","qualitative = [i for i in train.columns if train.dtypes[i] == 'object']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.86399Z","iopub.status.busy":"2021-07-28T18:41:02.863716Z","iopub.status.idle":"2021-07-28T18:41:02.869606Z","shell.execute_reply":"2021-07-28T18:41:02.868454Z","shell.execute_reply.started":"2021-07-28T18:41:02.863964Z"},"trusted":true},"outputs":[],"source":["print(list(quantitative))\n","print()\n","print('Train dataframe has: ', len(quantitative), 'quantitative features')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.871349Z","iopub.status.busy":"2021-07-28T18:41:02.870884Z","iopub.status.idle":"2021-07-28T18:41:02.8815Z","shell.execute_reply":"2021-07-28T18:41:02.880443Z","shell.execute_reply.started":"2021-07-28T18:41:02.871254Z"},"trusted":true},"outputs":[],"source":["print(list(qualitative))\n","print()\n","print('Train dataframe has: ', len(qualitative), 'qualitative features')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.88347Z","iopub.status.busy":"2021-07-28T18:41:02.882936Z","iopub.status.idle":"2021-07-28T18:41:02.891835Z","shell.execute_reply":"2021-07-28T18:41:02.891023Z","shell.execute_reply.started":"2021-07-28T18:41:02.883428Z"},"trusted":true},"outputs":[],"source":["print(train['Neighborhood'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.893256Z","iopub.status.busy":"2021-07-28T18:41:02.892882Z","iopub.status.idle":"2021-07-28T18:41:02.970649Z","shell.execute_reply":"2021-07-28T18:41:02.969614Z","shell.execute_reply.started":"2021-07-28T18:41:02.893229Z"},"trusted":true},"outputs":[],"source":["# Encode categorical features as ordered numbers\n","train = train.replace({'MSZoning' : {'RL': 1, 'RM': 2, 'C (all)': 3, 'FV': 4, 'RH': 5}, \n","                       'Street' : {'Grvl' : 1, 'Pave' : 2}, \n","                       'LotShape' : {'IR3' : 1, 'IR2' : 2, 'IR1' : 3, 'Reg' : 4}, \n","                       'LandContour': {'Lvl': 1, 'Bnk': 2, 'Low': 3, 'HLS': 4}, \n","                       'Utilities' : {'ELO' : 1, 'NoSeWa' : 2, 'NoSewr' : 3, 'AllPub' : 4}, \n","                       'LotConfig' : {'Inside': 1, 'FR2': 2, 'Corner': 3, 'CulDSac': 4, 'FR3': 5}, \n","                       'LandSlope' : {'Sev' : 1, 'Mod' : 2, 'Gtl' : 3}, \n","                       'Neighborhood' : {'CollgCr' : 1, 'OldTown' : 2, 'Edwards' : 3, \n","                                         'Somerst' : 4, 'Gilbert' : 5, 'NridgHt' : 6, \n","                                         'Sawyer' : 7, 'NWAmes' : 8, 'SawyerW' : 9, \n","                                         'BrkSide' : 10, 'Crawfor' : 11, 'Mitchel' : 12, \n","                                         'NoRidge' : 13, 'Timber' : 14, 'IDOTRR' : 15, \n","                                         'ClearCr' : 16, 'StoneBr' : 17, 'SWISU' : 18, \n","                                         'Blmngtn' : 19, 'MeadowV' : 20, 'BrDale' : 21, \n","                                         'Veenker' : 22, 'NPkVill' : 23, 'Blueste' : 24, \n","                                         'NAmes' : 25}, \n","                       'Condition1' : {'Norm': 1, 'Feedr': 2, 'PosN': 3, 'Artery': 4, 'RRAe': 5, \n","                                       'RRNn': 6, 'RRAn': 7, 'PosA': 8, 'RRNe': 9}, \n","                       'Condition2' : {'Norm': 1, 'Artery': 2, 'RRNn': 3, 'Feedr': 4, 'PosN': 5,\n","                                       'PosA': 6, 'RRAn': 7, 'RRAe': 8}, \n","                       'BldgType' : {'1Fam': 1, '2fmCon': 2, 'Duplex': 3, 'TwnhsE': 4, 'Twnhs': 5}, \n","                       'HouseStyle' : {'2Story': 1, '1Story': 2, '1.5Fin': 3, '1.5Unf': 4, 'SFoyer': 5, \n","                                       'SLvl': 6, '2.5Unf': 7, '2.5Fin': 8}, \n","                       'RoofStyle' : {'Gable': 1, 'Hip': 2, 'Gambrel': 3, 'Mansard': 4, 'Flat': 5, \n","                                      'Shed': 6}, \n","                       'RoofMatl' : {'CompShg': 1, 'WdShngl': 2, 'Metal': 3, 'WdShake': 4, 'Membran': 5, \n","                                     'Tar&Grv': 6, 'Roll': 7, 'ClyTile': 8}, \n","                       'Exterior1st' : {'VinylSd': 1, 'MetalSd': 2, 'Wd Sdng': 3, 'HdBoard': 4, 'BrkFace': 5, \n","                                        'WdShing': 6, 'CemntBd': 7, 'Plywood': 8, 'AsbShng': 9, 'Stucco': 10, \n","                                        'BrkComm': 11, 'AsphShn': 12, 'Stone': 13, 'ImStucc': 14, 'CBlock': 15},\n","                       'Exterior2nd' : {'VinylSd': 1, 'MetalSd': 2, 'Wd Shng': 3, 'HdBoard': 4, 'Plywood': 5, \n","                                        'Wd Sdng': 6, 'CmentBd': 7, 'BrkFace': 8, 'Stucco': 9, 'AsbShng': 10, \n","                                        'Brk Cmn': 11, 'ImStucc': 12, 'AsphShn': 13, 'Stone': 14, 'Other': 15, \n","                                        'CBlock': 16}, \n","                       'ExterQual' : {'Po' : 1, 'Fa' : 2, 'TA': 3, 'Gd': 4, 'Ex' : 5},\n","                       'ExterCond' : {'Po' : 1, 'Fa' : 2, 'TA': 3, 'Gd': 4, 'Ex' : 5},\n","                       'Foundation' : {'PConc': 1, 'CBlock': 2, 'BrkTil': 3, 'Wood': 4, 'Slab': 5, \n","                                       'Stone': 6}, \n","                       'Heating' : {'GasA': 1, 'GasW': 2, 'Grav': 3, 'Wall': 4, 'OthW': 5, \n","                                    'Floor': 6}, \n","                       'HeatingQC' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n","                       'CentralAir' : {'Y': 1, 'N': 2}, \n","                       'Electrical' : {'SBrkr': 1, 'FuseF': 2, 'FuseA': 3, 'FuseP': 4, 'Mix': 5}, \n","                       'KitchenQual' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n","                       'Functional' : {'Sal' : 1, 'Sev' : 2, 'Maj2' : 3, 'Maj1' : 4, 'Mod': 5, \n","                                       'Min2' : 6, 'Min1' : 7, 'Typ' : 8},\n","                       'PavedDrive' : {'N' : 1, 'P' : 2, 'Y' : 3}, \n","                       'SaleType' : {'WD': 1, 'New': 2, 'COD': 3, 'ConLD': 4, 'ConLI': 5, \n","                                     'CWD': 6, 'ConLw': 7, 'Con': 8, 'Oth': 9}, \n","                       'SaleCondition' : {'Normal': 1, 'Abnorml': 2, 'Partial': 3, 'AdjLand': 4, 'Alloca': 5, \n","                                          'Family': 6}})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:02.975121Z","iopub.status.busy":"2021-07-28T18:41:02.974813Z","iopub.status.idle":"2021-07-28T18:41:05.53533Z","shell.execute_reply":"2021-07-28T18:41:05.534591Z","shell.execute_reply.started":"2021-07-28T18:41:02.975092Z"},"trusted":true},"outputs":[],"source":["# Correlation matrix/heatmap after encoding\n","correlation = train.corr()\n","f, ax = plt.subplots(figsize=(20, 20))\n","sns.heatmap(correlation, vmax=.8, \n","            cmap=sns.diverging_palette(20, 220, n=200), \n","            square=True)\n","ax.set_xticklabels(\n","    ax.get_xticklabels(),\n","    rotation=45,\n","    horizontalalignment='right');"]},{"cell_type":"markdown","metadata":{},"source":["Looking at this heat map, it is evident that OverallQual, GrLivArea, ExterQual, GarageCars, and GarageArea have strong relationships with SalePrice. Let's zoom into the features with the highest correlations."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:05.537406Z","iopub.status.busy":"2021-07-28T18:41:05.537006Z","iopub.status.idle":"2021-07-28T18:41:07.06759Z","shell.execute_reply":"2021-07-28T18:41:07.066752Z","shell.execute_reply.started":"2021-07-28T18:41:05.53737Z"},"trusted":true},"outputs":[],"source":["# Zoomed-in heatmap\n","n = 15\n","cols = correlation.nlargest(n, 'SalePrice')['SalePrice'].index\n","correlation = np.corrcoef(train[cols].values.T)\n","f, ax = plt.subplots(figsize=(15, 15))\n","sns.heatmap(correlation, cbar=True, annot=True, \n","                 cmap=sns.diverging_palette(20, 220, n=200), \n","                 square=True, \n","                 fmt='.2f', annot_kws={'size': 15}, \n","                 yticklabels=cols.values, \n","                 xticklabels=cols.values)\n","ax.set_xticklabels(\n","    ax.get_xticklabels(),\n","    rotation=45,\n","    horizontalalignment='right');"]},{"cell_type":"markdown","metadata":{},"source":["The most important features in determining SalePrice are OverallQual, GrLivArea, ExterQual, KitchenQual, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt, and YearRemodAdd. Here, I am defining 'important features' as those having correlations greater than 0.50. So far, four out of six (YearBuilt, OverallQual, GrLivArea, and KitchenQual) of the features in my prediction have been shown to be moderately to highly correlated."]},{"cell_type":"markdown","metadata":{},"source":["# Examing Scatter Plots for Outliers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:07.069048Z","iopub.status.busy":"2021-07-28T18:41:07.068776Z","iopub.status.idle":"2021-07-28T18:41:07.079846Z","shell.execute_reply":"2021-07-28T18:41:07.078755Z","shell.execute_reply.started":"2021-07-28T18:41:07.069019Z"},"trusted":true},"outputs":[],"source":["# Differentiate numerical features (minus the target) and categorical features\n","categorical_features = train.select_dtypes(include = ['object']).columns\n","numerical_features = train.select_dtypes(exclude = ['object']).columns\n","numerical_features = numerical_features.drop('SalePrice')\n","print('Numerical features : ' + str(len(numerical_features)))\n","print('Categorical features : ' + str(len(categorical_features)))\n","train_num = train[numerical_features]\n","train_cat = train[categorical_features]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:07.081222Z","iopub.status.busy":"2021-07-28T18:41:07.080967Z","iopub.status.idle":"2021-07-28T18:41:07.09308Z","shell.execute_reply":"2021-07-28T18:41:07.091974Z","shell.execute_reply.started":"2021-07-28T18:41:07.081197Z"},"trusted":true},"outputs":[],"source":["cols_to_eval = ['SalePrice', 'OverallQual', 'GrLivArea', 'ExterQual', \n","                'KitchenQual', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'FullBath', \n","                'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd', 'Fireplaces', 'HeatingQC', 'Neighborhood']\n","\n","cols_to_eval1 = ['SalePrice', 'OverallQual', 'GrLivArea', 'ExterQual']\n","\n","cols_to_eval2 = ['SalePrice', 'KitchenQual', 'GarageCars', 'GarageArea']\n","\n","cols_to_eval3 = ['SalePrice', 'TotalBsmtSF', '1stFlrSF', 'FullBath']\n","\n","cols_to_eval4 = ['SalePrice', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']\n","\n","cols_to_eval5 = ['SalePrice', 'Fireplaces', 'HeatingQC', 'Neighborhood']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:07.094791Z","iopub.status.busy":"2021-07-28T18:41:07.094433Z","iopub.status.idle":"2021-07-28T18:41:10.60128Z","shell.execute_reply":"2021-07-28T18:41:10.600174Z","shell.execute_reply.started":"2021-07-28T18:41:07.094763Z"},"trusted":true},"outputs":[],"source":["sns.set()\n","\n","sns.pairplot(train[cols_to_eval1], height = 4, aspect = 1.2)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:10.603346Z","iopub.status.busy":"2021-07-28T18:41:10.60291Z","iopub.status.idle":"2021-07-28T18:41:14.402645Z","shell.execute_reply":"2021-07-28T18:41:14.401799Z","shell.execute_reply.started":"2021-07-28T18:41:10.603287Z"},"trusted":true},"outputs":[],"source":["sns.set()\n","\n","sns.pairplot(train[cols_to_eval2], height = 4, aspect = 1.2)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:14.404089Z","iopub.status.busy":"2021-07-28T18:41:14.403691Z","iopub.status.idle":"2021-07-28T18:41:17.716206Z","shell.execute_reply":"2021-07-28T18:41:17.715303Z","shell.execute_reply.started":"2021-07-28T18:41:14.404048Z"},"trusted":true},"outputs":[],"source":["sns.set()\n","\n","sns.pairplot(train[cols_to_eval3], height = 4, aspect = 1.2)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:17.71765Z","iopub.status.busy":"2021-07-28T18:41:17.717385Z","iopub.status.idle":"2021-07-28T18:41:21.423301Z","shell.execute_reply":"2021-07-28T18:41:21.422146Z","shell.execute_reply.started":"2021-07-28T18:41:17.717623Z"},"trusted":true},"outputs":[],"source":["sns.set()\n","\n","sns.pairplot(train[cols_to_eval4], height = 4, aspect = 1.2)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:21.425577Z","iopub.status.busy":"2021-07-28T18:41:21.425102Z","iopub.status.idle":"2021-07-28T18:41:25.12557Z","shell.execute_reply":"2021-07-28T18:41:25.124364Z","shell.execute_reply.started":"2021-07-28T18:41:21.425531Z"},"trusted":true},"outputs":[],"source":["sns.set()\n","\n","sns.pairplot(train[cols_to_eval5], height = 4, aspect = 1.2)\n","plt.show();"]},{"cell_type":"markdown","metadata":{},"source":["# Handling Outliers\n","\n","In general, a skewness with absolute value > 0.5 is considered at least moderately skewed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:25.127154Z","iopub.status.busy":"2021-07-28T18:41:25.126864Z","iopub.status.idle":"2021-07-28T18:41:25.164027Z","shell.execute_reply":"2021-07-28T18:41:25.163117Z","shell.execute_reply.started":"2021-07-28T18:41:25.127126Z"},"trusted":true},"outputs":[],"source":["from scipy.stats import skew\n","\n","# Log transform skewed numerical features to lessen impact of outliers\n","cols_quant = ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', \n","              'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n","              'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', \n","              'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n","              'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n","              'MiscVal', 'MoSold', 'YrSold']\n","skewness = train[cols_quant].apply(lambda x: skew(x))\n","skewness = skewness[abs(skewness) > 0.5]\n","print(str(skewness.shape[0]) + \" skewed numerical features were log transformed\")\n","skewed_features = skewness.index\n","train[skewed_features] = np.log1p(train[skewed_features])"]},{"cell_type":"markdown","metadata":{},"source":["# Analyzing SalePrice"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:25.165911Z","iopub.status.busy":"2021-07-28T18:41:25.165318Z","iopub.status.idle":"2021-07-28T18:41:26.700938Z","shell.execute_reply":"2021-07-28T18:41:26.699689Z","shell.execute_reply.started":"2021-07-28T18:41:25.165856Z"},"trusted":true},"outputs":[],"source":["# Check for normality\n","plt.figure(1); plt.title('Normal')\n","sns.distplot(train['SalePrice'], kde=False, fit=stats.norm)\n","\n","plt.figure(2); plt.title('Log Normal')\n","sns.distplot(train['SalePrice'], kde=False, fit=stats.lognorm)\n","\n","plt.figure(3); plt.title('Johnson SU')\n","sns.distplot(train['SalePrice'], kde=False, fit=stats.johnsonsu)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:26.702737Z","iopub.status.busy":"2021-07-28T18:41:26.702274Z","iopub.status.idle":"2021-07-28T18:41:26.913075Z","shell.execute_reply":"2021-07-28T18:41:26.912299Z","shell.execute_reply.started":"2021-07-28T18:41:26.702705Z"},"trusted":true},"outputs":[],"source":["# Probability plot\n","res = stats.probplot(train['SalePrice'], plot=plt)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:26.914204Z","iopub.status.busy":"2021-07-28T18:41:26.913965Z","iopub.status.idle":"2021-07-28T18:41:26.925021Z","shell.execute_reply":"2021-07-28T18:41:26.923698Z","shell.execute_reply.started":"2021-07-28T18:41:26.91418Z"},"trusted":true},"outputs":[],"source":["# Summary statistics\n","train['SalePrice'].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:26.92742Z","iopub.status.busy":"2021-07-28T18:41:26.926953Z","iopub.status.idle":"2021-07-28T18:41:26.934833Z","shell.execute_reply":"2021-07-28T18:41:26.933653Z","shell.execute_reply.started":"2021-07-28T18:41:26.927352Z"},"trusted":true},"outputs":[],"source":["print(\"Skewness: %.02f\" % train['SalePrice'].skew())\n","print(\"Kurtosis: %.02f\" % train['SalePrice'].kurt())"]},{"cell_type":"markdown","metadata":{},"source":["SalePrice does not have a normal distribution, so it will need to be transformed before performing regression. For SalePrice, the best fit is the Unbounded Johnson Distribution as seen above. Log transformation can smooth out the data, removing the skew, to make it more 'normal', which will allow the statistical analysis results from these data to be more valid."]},{"cell_type":"markdown","metadata":{},"source":["# Standardizing Data\n","\n","Standardization will convert the data to have a mean of 0 and a standard deviation of 1. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:26.936549Z","iopub.status.busy":"2021-07-28T18:41:26.936245Z","iopub.status.idle":"2021-07-28T18:41:26.949754Z","shell.execute_reply":"2021-07-28T18:41:26.948419Z","shell.execute_reply.started":"2021-07-28T18:41:26.936521Z"},"trusted":true},"outputs":[],"source":["# Standardizing SalePrice\n","saleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:,np.newaxis]);\n","low_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\n","high_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\n","print('outer range (low) of the distribution:')\n","print(low_range)\n","print('\\nouter range (high) of the distribution:')\n","print(high_range)"]},{"cell_type":"markdown","metadata":{},"source":["# Bivariate Analysis\n","\n","My prediction was that YearBuilt, OverallQual, OverallCond, Neighborhood, GrLivArea, and KitchenQual play the biggest roles in determining SalePrice. Let's take a closer look at these relationships with bivariate analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:26.95145Z","iopub.status.busy":"2021-07-28T18:41:26.951041Z","iopub.status.idle":"2021-07-28T18:41:27.202587Z","shell.execute_reply":"2021-07-28T18:41:27.201635Z","shell.execute_reply.started":"2021-07-28T18:41:26.9514Z"},"trusted":true},"outputs":[],"source":["# Bivariate analysis SalePrice/YearBuilt\n","var = 'YearBuilt'\n","data = pd.concat([train['SalePrice'], train[var]], axis=1)\n","data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"]},{"cell_type":"markdown","metadata":{},"source":["SalePrice and YearBuilt have a somewhat linear positive relationship to each other with a shallow slope. This scatterplot reveals that these two variables are not strongly related. Some of the years have vertically clustered data points. This could be due to innacurate records since they tend to occur at 1900, 1920, 1940, etc. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:27.203988Z","iopub.status.busy":"2021-07-28T18:41:27.20374Z","iopub.status.idle":"2021-07-28T18:41:27.425586Z","shell.execute_reply":"2021-07-28T18:41:27.424413Z","shell.execute_reply.started":"2021-07-28T18:41:27.203963Z"},"trusted":true},"outputs":[],"source":["# Bivariate analysis SalePrice/OverallQual\n","var = 'OverallQual'\n","data = pd.concat([train['SalePrice'], train[var]], axis=1)\n","data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"]},{"cell_type":"markdown","metadata":{},"source":["There is a positive, fainlty linear trend between SalePrice and OverallQual whose relationship weakens as OverallQual increases and has a gentle slope."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:27.427289Z","iopub.status.busy":"2021-07-28T18:41:27.426972Z","iopub.status.idle":"2021-07-28T18:41:27.671765Z","shell.execute_reply":"2021-07-28T18:41:27.67076Z","shell.execute_reply.started":"2021-07-28T18:41:27.427247Z"},"trusted":true},"outputs":[],"source":["# Bivariate analysis SalePrice/OverallCond\n","var = 'OverallCond'\n","data = pd.concat([train['SalePrice'], train[var]], axis=1)\n","data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"]},{"cell_type":"markdown","metadata":{},"source":["There is a positive, slightly nonlinear trend between SalePrice and OverallQual. In general, as OverallQual increases, SalePrice increases. Because OverallQual is described using discrete values, data points are represented as vertically clustered lines at each x-value. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:27.673299Z","iopub.status.busy":"2021-07-28T18:41:27.673019Z","iopub.status.idle":"2021-07-28T18:41:28.0983Z","shell.execute_reply":"2021-07-28T18:41:28.097377Z","shell.execute_reply.started":"2021-07-28T18:41:27.67327Z"},"trusted":true},"outputs":[],"source":["# Bivariate analysis SalePrice/Neighborhood\n","var = 'Neighborhood'\n","data = pd.concat([train['SalePrice'], train[var]], axis=1)\n","data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"]},{"cell_type":"markdown","metadata":{},"source":["There does not appear to be a trend between SalePrice and Neighborhood. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:28.099731Z","iopub.status.busy":"2021-07-28T18:41:28.099469Z","iopub.status.idle":"2021-07-28T18:41:28.322786Z","shell.execute_reply":"2021-07-28T18:41:28.321785Z","shell.execute_reply.started":"2021-07-28T18:41:28.099704Z"},"trusted":true},"outputs":[],"source":["# Bivariate analysis SalePrice/GRLivArea\n","var = 'GrLivArea'\n","data = pd.concat([train['SalePrice'], train[var]], axis=1)\n","data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"]},{"cell_type":"markdown","metadata":{},"source":["There is a positive, linear trend between SalePrice and GrLivArea whose relationship weakens as GrLivArea increases and has a moderately steep slope. There are some outliers above 4,000 square feet. The two that have a SalePrice of greater than 700,000 may be located in an area that is highly desireable. However, they follow the overall trend of the data, so they will remain in the analysis. The two that have a SalePrice of less than 200,000 may be located in rural areas or areas that are not desireable. These two data points are not representative of the rest of the dataset, so these can be removed to avoid errors in the analysis.  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:28.325273Z","iopub.status.busy":"2021-07-28T18:41:28.325009Z","iopub.status.idle":"2021-07-28T18:41:28.557225Z","shell.execute_reply":"2021-07-28T18:41:28.556241Z","shell.execute_reply.started":"2021-07-28T18:41:28.325246Z"},"trusted":true},"outputs":[],"source":["# Bivariate analysis SalePrice/KitchenQual\n","var = 'KitchenQual'\n","data = pd.concat([train['SalePrice'], train[var]], axis=1)\n","data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));"]},{"cell_type":"markdown","metadata":{},"source":["There is a positive, linear trend between SalePrice and KitchenQual though the degree of scatter indicates a weak relationship that continues to weaken as KitchenQual increases. "]},{"cell_type":"markdown","metadata":{},"source":["A closer look at these variables and their relationship to SalePrice shows that these variables are not as closely related as one might first intuit. Of the initial predictions, OverallQual, GrLivArea, and KitchenQual have the strongest relationship with SalePrice, though the relationships all seem to weaken as these features increase. It is surprising the YearBuilt and Neigborhood don't have a stronger relationship. The relationship seen here between SalePrice and YearBuilt can be explained by the fact that people tend to remodel homes and make improvements throughout their occupancy. Given that the Neighborhood feature is represented here in a nominal manner (no ordering is possible or implied in the levels), a relationship does not emerge in this analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:28.559031Z","iopub.status.busy":"2021-07-28T18:41:28.558632Z","iopub.status.idle":"2021-07-28T18:41:28.568633Z","shell.execute_reply":"2021-07-28T18:41:28.567346Z","shell.execute_reply.started":"2021-07-28T18:41:28.558989Z"},"trusted":true},"outputs":[],"source":["# Delete outliers\n","train = train[train.GrLivArea < 4500]\n","train.reset_index(drop=True, inplace=True)\n","\n","train = train[train.TotalBsmtSF < 3000]\n","train.reset_index(drop=True, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Normality\n","\n","Normality for SalePrice has already been examined by visualizing its distribution plot, examining skew and kurtosis, and observing its probability plot. It is not normal, shows peakedness, positive skewness, and does not follow a diagonal line in its probability plot. Log transformations can help with positive skewness."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:28.570828Z","iopub.status.busy":"2021-07-28T18:41:28.570257Z","iopub.status.idle":"2021-07-28T18:41:28.581683Z","shell.execute_reply":"2021-07-28T18:41:28.580661Z","shell.execute_reply.started":"2021-07-28T18:41:28.570795Z"},"trusted":true},"outputs":[],"source":["# Log transformation\n","train['SalePrice'] = np.log(train['SalePrice'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:28.583526Z","iopub.status.busy":"2021-07-28T18:41:28.583049Z","iopub.status.idle":"2021-07-28T18:41:29.114607Z","shell.execute_reply":"2021-07-28T18:41:29.113249Z","shell.execute_reply.started":"2021-07-28T18:41:28.583486Z"},"trusted":true},"outputs":[],"source":["# Visualize transformed histogram and normal probability plot\n","sns.distplot(train['SalePrice'], fit=stats.norm)\n","plt.figure()\n","\n","res = stats.probplot(train['SalePrice'], plot=plt)"]},{"cell_type":"markdown","metadata":{},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{},"source":["Preparing data for modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:29.11655Z","iopub.status.busy":"2021-07-28T18:41:29.116125Z","iopub.status.idle":"2021-07-28T18:41:29.141377Z","shell.execute_reply":"2021-07-28T18:41:29.140374Z","shell.execute_reply.started":"2021-07-28T18:41:29.116509Z"},"trusted":true},"outputs":[],"source":["# Combine categorical and numerical features\n","cols_for_mod = ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', \n","                'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n","                'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', \n","                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n","                'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', \n","                'MiscVal', 'MoSold', 'YrSold', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', \n","                'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', \n","                'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', \n","                'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'PavedDrive', \n","                'SaleType', 'SaleCondition', 'SalePrice']\n","\n","train = train[cols_for_mod]\n","\n","print('Shape of dataframe: ' + str(train.shape))\n","print()\n","print('Column names: ' + str(list(train.columns)))\n","print()\n","\n","# DataFrame with features only\n","train_fr = train.copy()\n","train_fr.drop(['SalePrice'], inplace = True, axis = 1)\n","\n","# Segregate features and labels into separate variables\n","X,y = train_fr, train['SalePrice']\n","\n","print('Number of independent variables: ' + str(X.shape[1]))\n","print()\n","\n","from sklearn.model_selection import cross_val_score, train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n","\n","# Standardize features\n","std = StandardScaler()\n","X = std.fit_transform(X)\n","\n","# Partition the dataset in train + validation sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n","\n","print('X_train : ' + str(X_train.shape))\n","print('X_test : ' + str(X_test.shape))\n","print('y_train : ' + str(y_train.shape))\n","print('y_test : ' + str(y_test.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:29.142961Z","iopub.status.busy":"2021-07-28T18:41:29.142672Z","iopub.status.idle":"2021-07-28T18:41:29.149134Z","shell.execute_reply":"2021-07-28T18:41:29.147449Z","shell.execute_reply.started":"2021-07-28T18:41:29.142933Z"},"trusted":true},"outputs":[],"source":["# RMSE\n","def rmse_cv_train(model):\n","    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv = 5))\n","    return(rmse)\n","\n","def rmse_cv_test(model):\n","    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring='neg_mean_squared_error', cv = 5))\n","    return(rmse)"]},{"cell_type":"markdown","metadata":{},"source":["### Linear Regression"]},{"cell_type":"markdown","metadata":{},"source":["Regression is a supervised learning task used to predict a target numeric value from a set of related features. Linear regression assumes a linear relationship between the target and inputs."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:29.152048Z","iopub.status.busy":"2021-07-28T18:41:29.151513Z","iopub.status.idle":"2021-07-28T18:41:29.811007Z","shell.execute_reply":"2021-07-28T18:41:29.809862Z","shell.execute_reply.started":"2021-07-28T18:41:29.15199Z"},"trusted":true},"outputs":[],"source":["# Linear Regression Without Regularization\n","\n","lr = LinearRegression()\n","lr.fit(X_train, y_train)\n","\n","# Look at predictions on training and validation set\n","print('RMSE on Train set :', rmse_cv_train(lr).mean())\n","print('RMSE on Test set :', rmse_cv_test(lr).mean())\n","\n","y_train_pred = lr.predict(X_train)\n","y_test_pred = lr.predict(X_test)\n","\n","# Plot residuals\n","plt.scatter(y_train_pred, y_train_pred - y_train, c = 'blue', marker = 's', label = 'Train data')\n","plt.scatter(y_test_pred, y_test_pred - y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\n","plt.title('Linear Regression')\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.legend(loc = 'upper left')\n","plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = 'red')\n","plt.show()\n","\n","# Plot predictions\n","plt.scatter(y_train_pred, y_train, c = 'blue', marker = 's', label = 'Train data')\n","plt.scatter(y_test_pred, y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\n","plt.title('Linear Regression')\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Real Values')\n","plt.legend(loc = 'upper left')\n","plt.plot([10.5, 13.5], [10.5, 13.5], c = 'red')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Residuals are the difference between the observed value of the target value and the predicted value (the error of prediction). In the residual plot, the predicted values are on the x-axis and the error on the y-axis. The residuals are fairly randomly and uniformly disbursed around the zero line (meaning a linear regresion model is appropriate for the data) and cluster around it, so the model is performing well. \n","\n","The Root Mean Squared Error (RMSE) score is pretty low on the Train set, but high in the Test set. The RMSE gives an estimate of spread of observed data points across the predicted regression line. There is a large difference between the RMSE on Train set (0.2058753430825945) and the RMSE on Test set (30350035354.640392), so it looks like the model is overfitting. The low RMSE score on the Training set shows that it is fitting the data well there. Let's take a look at other models to see if we can see improved performance."]},{"cell_type":"markdown","metadata":{},"source":["### LASSO Regression"]},{"cell_type":"markdown","metadata":{},"source":["Regularized regression adds penalties to the loss function during training, which constrains the coefficient estimates to zero. The loss function is a measure that indicates how well the model’s coefficients have fit the underlying data. Regularization assists with avoiding overfitting and reduces model complexity, which can occur in simple linear regression. It is a useful method for handling collinearity and filter out noise from data.\n","\n","LASSO regression (L1 regularization) limits the size of the coefficient by adding an L1 penalty equal to the absolute value of the magnitude of coefficients. That is, we sum the absolute value of the weights. This type of regression uses shrinkage, which is where data values are shrunk towards a central point such as the mean. LASSO stands for Least Absolute Shrinkage and Selection Operator. LASSO regression can help with feature selection as it can lead to zero coefficients for some of the features, which means they will be completely neglected for the evaluation of the output. This is useful when we have a high dimensional dataset with features that are irrelevant."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:29.812674Z","iopub.status.busy":"2021-07-28T18:41:29.81226Z","iopub.status.idle":"2021-07-28T18:41:29.826552Z","shell.execute_reply":"2021-07-28T18:41:29.825731Z","shell.execute_reply.started":"2021-07-28T18:41:29.812642Z"},"trusted":true},"outputs":[],"source":["X_df = pd.DataFrame(X_train, columns = train_fr.columns)\n","\n","print(X_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:29.827927Z","iopub.status.busy":"2021-07-28T18:41:29.827566Z","iopub.status.idle":"2021-07-28T18:41:31.612683Z","shell.execute_reply":"2021-07-28T18:41:31.611578Z","shell.execute_reply.started":"2021-07-28T18:41:29.8279Z"},"trusted":true},"outputs":[],"source":["# LASSO Regression With L1 Regularization\n","\n","lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n","                          0.3, 0.6, 1], \n","                max_iter = 50000, cv = 10)\n","lasso.fit(X_train, y_train)\n","alpha = lasso.alpha_\n","print('Best alpha :', alpha)\n","\n","print('More precision with alphas centered around ' + str(alpha))\n","lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n","                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n","                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n","                          alpha * 1.4], \n","                max_iter = 50000, cv = 10)\n","lasso.fit(X_train, y_train)\n","alpha = lasso.alpha_\n","print('Best alpha :', alpha)\n","\n","print('LASSO RMSE on Train set :', rmse_cv_train(lasso).mean())\n","print('LASSO RMSE on Test set :', rmse_cv_test(lasso).mean())\n","y_train_las = lasso.predict(X_train)\n","y_test_las = lasso.predict(X_test)\n","\n","# Plot residuals\n","plt.scatter(y_train_las, y_train_las - y_train, c = 'blue', marker = 's', label = 'Train data')\n","plt.scatter(y_test_las, y_test_las - y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\n","plt.title('Linear Regression with LASSO Regularization')\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.legend(loc = 'upper left')\n","plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = 'red')\n","plt.show()\n","\n","# Plot predictions\n","plt.scatter(y_train_las, y_train, c = 'blue', marker = 's', label = 'Train data')\n","plt.scatter(y_test_las, y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\n","plt.title('Linear Regression with LASSO Regularization')\n","plt.xlabel('Predicted values')\n","plt.ylabel('Real values')\n","plt.legend(loc = 'upper left')\n","plt.plot([10.5, 13.5], [10.5, 13.5], c = 'red')\n","plt.show()\n","\n","# Plot important coefficients\n","coefs = pd.Series(lasso.coef_, index = X_df.columns)\n","print('LASSO picked ' + str(sum(coefs != 0)) + ' features and eliminated ' +  \\\n","      str(sum(coefs == 0)) + ' features')\n","imp_coefs = pd.concat([coefs.sort_values().head(10),\n","                     coefs.sort_values().tail(10)])\n","imp_coefs.plot(kind = 'barh')\n","plt.title('Coefficients in the LASSO Model')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The RMSE scores for LASSO regression show quite an improvement! The RMSE score on the Train set is 0.13368814791535225 and 0.15266185052830566 for the Test set. In addition, LASSO picked 41 features and eliminated 20 features. Alpha is the parameter that balances the amount of emphasis given to minimizing the residual sum of squares vs minimizing sum of square of coefficients.\n","\n","The highest weights have been assigned to GrLivArea, OverallQual, YearBuilt, 1stFlrSF, and LotArea. This is somewaht in line with the earlier predictions that YearBuilt, OverallQual, and GrLivArea play the biggest roles in determining SalePrice! The features LASSO picked make sense on an intuitive level."]},{"cell_type":"markdown","metadata":{},"source":["### Ridge Regression"]},{"cell_type":"markdown","metadata":{},"source":["Ridge regression is used for estimating the coefficients where independent variables are highly correlated. The cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients. The cost function can be used to show how badly a model is performing. Ridge regression helps understand the degree of overfitting of the data by measuring the magnitude of the coefficients."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:31.617536Z","iopub.status.busy":"2021-07-28T18:41:31.617226Z","iopub.status.idle":"2021-07-28T18:41:38.795797Z","shell.execute_reply":"2021-07-28T18:41:38.795122Z","shell.execute_reply.started":"2021-07-28T18:41:31.617505Z"},"trusted":true},"outputs":[],"source":["# Ridge Regression With L2 Regularization\n","\n","ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\n","ridge.fit(X_train, y_train)\n","alpha = ridge.alpha_\n","print('Best alpha :', alpha)\n","\n","print('More precision with alphas centered around ' + str(alpha))\n","ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n","                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n","                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n","                cv = 10)\n","ridge.fit(X_train, y_train)\n","alpha = ridge.alpha_\n","print('Best alpha :', alpha)\n","\n","print('Ridge RMSE on Train set :', rmse_cv_train(ridge).mean())\n","print('Ridge RMSE on Test set :', rmse_cv_test(ridge).mean())\n","y_train_rdg = ridge.predict(X_train)\n","y_test_rdg = ridge.predict(X_test)\n","\n","# Plot residuals\n","plt.scatter(y_train_rdg, y_train_rdg - y_train, c = 'blue', marker = 's', label = 'Train data')\n","plt.scatter(y_test_rdg, y_test_rdg - y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\n","plt.title('Linear Regression with Ridge Regularization')\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.legend(loc = 'upper left')\n","plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = 'red')\n","plt.show()\n","\n","# Plot predictions\n","plt.scatter(y_train_rdg, y_train, c = 'blue', marker = 's', label = 'Train data')\n","plt.scatter(y_test_rdg, y_test, c = 'lightgreen', marker = 's', label = 'Validation data')\n","plt.title('Linear Regression with Ridge Regularization')\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Real Values')\n","plt.legend(loc = 'upper left')\n","plt.plot([10.5, 13.5], [10.5, 13.5], c = 'red')\n","plt.show()\n","\n","# Plot important coefficients\n","coefs = pd.Series(ridge.coef_, index = X_df.columns)\n","print('Ridge picked ' + str(sum(coefs != 0)) + ' features and eliminated ' +  \\\n","      str(sum(coefs == 0)) + ' features')\n","imp_coefs = pd.concat([coefs.sort_values().head(10),\n","                     coefs.sort_values().tail(10)])\n","imp_coefs.plot(kind = 'barh')\n","plt.title('Coefficients in the Ridge Model')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The RMSE scores were slightly better with LASSO regression than with Ridge regression. The RMSE score on the Train set is 0.13582790751257817 and on the Test set is 0.15671943639084562 Ridge did not eliminate any features given that Ridge Regression does not make coefficients absolute zero whereas LASSO Regression will. Alpha is the parameter that balances the amount of emphasis given to minimizing the residual sum of squares vs minimizing sum of square of coefficients\n","\n","The features with the highest weights are OverallQual, GrLivArea, and 1stFlrSF."]},{"cell_type":"markdown","metadata":{},"source":["### Gradient Boosting and XGBoost"]},{"cell_type":"markdown","metadata":{},"source":["Gradient Boosting is an improvement on decision tress. New trees fit on an improved version of the training dataset by fitting the new predictor to the residual errors made by the previous predictor until no further improvements can be made. Decision tree-based algorithms are best for small to medium structured/tabular datasets.\n","\n","XGBoost is another variant of the Gradient Boost Machine algorithm that is higher-performing. It is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. XGBoost can be used for regression, classification, ranking, and user-defined problems."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-07-28T18:41:38.797138Z","iopub.status.busy":"2021-07-28T18:41:38.796779Z","iopub.status.idle":"2021-07-28T18:43:10.040136Z","shell.execute_reply":"2021-07-28T18:43:10.039397Z","shell.execute_reply.started":"2021-07-28T18:41:38.797111Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","\n","gbr = GradientBoostingRegressor(n_estimators=3500, \n","                                learning_rate=0.01, \n","                                max_depth=3, \n","                                max_features='sqrt', \n","                                min_samples_leaf=15, \n","                                min_samples_split=15, \n","                                loss='huber', \n","                                random_state=42)  \n","\n","xgboost = XGBRegressor(learning_rate=0.01, \n","                       n_estimators=3500, \n","                       max_depth=3, \n","                       min_child_weight=0, \n","                       gamma=0, \n","                       subsample=0.7, \n","                       colsample_bytree=0.7, \n","                       objective='reg:squarederror', \n","                       nthread=-1, \n","                       scale_pos_weight=1, \n","                       seed=42, \n","                       reg_alpha=0.00006)\n","\n","print('RMSE for Gradient Boosting on Train Set :', rmse_cv_train(gbr).mean())\n","print('RMSE for Gradient Boosting on Test Set :', rmse_cv_test(gbr).mean())\n","\n","print()\n","\n","print('RMSE for XGBoost on Train Set :', rmse_cv_train(xgboost).mean())\n","print('RMSE for XGBoost on Test Set :', rmse_cv_test(xgboost).mean())"]},{"cell_type":"markdown","metadata":{},"source":["The RMSE scores remain fairly consistent with those of the regression analysis. The Train versus Test RMSE scores remain close to each other, which shows that overfitting is not occurring. The following notes were used to tune the models.\n","\n","Tuning the Gradient Boosting Model:\n","- Tree-Specific Parameters -- affect each individual tree in the model.\n","    - max_depth: Used to control overfitting. Higher depths allow for the model to learn more about the relationships in the data. Typical values: 3-10\n","    - max_features: Generally, the square root of the total number of features works well, but checking 30-40% of the total number of features is a good idea. Higher values can lead to overfitting.\n","    - min_samples_leaf: Used to control overfitting.\n","    - min_samples_split: Used to control overfitting. The higher the minimum number of samples, the better the generalizations the model can make, however, too high can lead to underfitting since it is considering more samples at each node before splitting.\n","- Boosting Parameters -- affect the boosting operation in the model.\n","    - n_estimators: Should be balanced with the learning_rate. Higher numbers can overfit, but Gradient Boosting performs well with high numbers. Higher number of trees can also be computationally expensive.\n","    - learning_rate: Lower rates allow for the model to generalize well, but a higher number of trees will be needed. This should be balanced because a higher number of trees requires increasing computational power. Generally, 0.01-0.2 is the range used.\n","- Miscellaneous Parameters -- parameters for overall functioning.\n","    - loss: The loss function to be minimized at each split. \n","    - random_state: Needed so that same random numbers are generated every time for each parameter. It is helpful to keep the random_state constant across models so that performance metrics can be compared. \n","                                \n","Tuning the XGBoost Model:\n","- General Parameters -- overall functioning\n","    - nthread: Used for parallel processing and number of cores\n","- Booster Parameters -- guide the individual booster (tree/regression) at each step\n","    - n_estimators: Should be balanced with the learning_rate. Higher numbers can overfit, but Gradient Boosting performs well with high numbers. Higher number of trees can also be computationally expensive.\n","    - learning_rate (eta): Lower rates allow for the model to generalize well, but a higher number of trees will be needed. This should be balanced because a higher number of trees requires increasing computational power. Typical values: 0.01-0.2 \n","    - max_depth: Used to control overfitting. Higher depths allow for the model to learn more about the relationships in the data. Typical values: 3-10\n","    - min_child_weight: Similar to min_child_leaf in GBM. Used to control overfitting. Higher values help generalize, but too high could lead to underfitting. Choose small values if dealing with a highly imbalanced class problem.\n","    - gamma: Specifies the minimum loss reduction required to make a split. Makes the algorithm conservative.\n","    - subsample: The fraction of observations to randomly sample for each tree. Lower values prevent overfitting (makes the algorithm conservative), but too low could lead to underfitting and high generalization. Typical values: 0.5-1\n","    - colsample_bytree: Similar to max_features in GBM. Higher values can lead to overfitting. Typical values: 0.5-1\n","    - reg_alpha: L1 regularization term on weight (LASSO) and can be used in cases where there is high dimensionality\n","    - scale_pos_weight: A value greater than 0 can be used when there is high class imbalance. A value of 1 can be used in a case of high class imbalance.\n","- Learning Task Parameters -- guide the optimization performed\n","    - objective: Defines the loss function.\n","    - seed: Random number seed."]},{"cell_type":"markdown","metadata":{},"source":["# Conclusions\n","\n","LASSO and Ridge regression showed that OverallQual and GrLivArea are important features in predicting SalePrice. My initial prediction was that YearBuilt, OverallQual, OverallCond, Neighborhood, GrLivArea, KitchenQual were the features that would contribute most to SalePrice. Of my predictions, all but Neighborhood and KitchenQual were selected as important features in the various regression analyses. It was surprising that Neighborhood did not emerge as an important feature in the analyses. I believe this can be explained by the fact that the Neighborhood feature is represented in the dataset in a nominal manner (no ordering is possible or implied in the levels). A better analysis could be made if the feature were represented as neighborhood quality."]},{"cell_type":"markdown","metadata":{},"source":["# References\n","\n","Kaggle\n","- https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n","- https://www.kaggle.com/jesucristo/1-house-prices-solution-top-1?scriptVersionId=20214677\n","- https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset\n","- https://www.kaggle.com/dgawlik/house-prices-eda\n","\n","Medium\n","- https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9\n","- https://towardsdatascience.com/how-do-you-check-the-quality-of-your-regression-model-in-python-fa61759ff685\n","\n","Books\n","- Géron, Aurélien: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n","- Hair et al. (2013): https://amzn.to/2uC3j9p\n","\n","Other\n","- https://www.reneshbedre.com/blog/linear-regression.html\n","- https://www.scikit-yb.org/en/latest/api/regressor/residuals.html\n","- https://www.statisticshowto.com/regularized-regression/\n","- https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n","- https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1497212,"sourceId":2474071,"sourceType":"datasetVersion"}],"dockerImageVersionId":30120,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
